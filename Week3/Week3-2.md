# ðŸ“˜ Week3â€‘2 â€“ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  (MLP)

---

## ðŸ” ê¸°ë³¸ ê°œë… *(Multi-Layer Perceptron)*

### â–¶ï¸ í¼ì…‰íŠ¸ë¡ (Perceptron)
- ë‹¨ì¼ ìž…ë ¥ê°’ê³¼ ê°€ì¤‘ì¹˜ì˜ ê³±ì´ ìž„ê³„ê°’ì„ ë„˜ìœ¼ë©´ 1, ì•„ë‹ˆë©´ 0 ì¶œë ¥ (ì´ì§„ ë¶„ë¥˜)
- ë‹¨ì¼ ì¸µ ì‹ ê²½ë§ êµ¬ì¡°

### â–¶ï¸ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  (MLP)
- ìž…ë ¥ì¸µ(Input layer) â†’ ì€ë‹‰ì¸µ(Hidden Layer) â†’ ì¶œë ¥ì¸µ(Output layer)
- í•œ ê°œ ì´ìƒì˜ ì€ë‹‰ì¸µì´ ì¡´ìž¬í•˜ëŠ” ì‹ ê²½ë§ (ë¹„ì„ í˜• íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥)

### â–¶ï¸ PyTorch êµ¬ì¡° ì˜ˆì‹œ
- í•œ LayerëŠ” Linear(ì„ í˜•) ì—°ì‚° + ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ ì¡°í•©

```python
from torch import nn
model = nn.Sequential(
    nn.Linear(2, 4),   # input: 2 features â†’ hidden: 4 units
    nn.ReLU(),
    nn.Linear(4, 1),   # hidden: 4 units â†’ output: 1
)
```

---

## ðŸ”¢ ì‹ ê²½ë§ Forward ì—°ì‚°

- ê° Layerì—ì„œ ìž…ë ¥ê°’ê³¼ ê°€ì¤‘ì¹˜ì˜ ì„ í˜• ê²°í•© ê²°ê³¼ê°€ ë¹„ì„ í˜• í•¨ìˆ˜(ReLU, Sigmoid ë“±)ë¥¼ ê±°ì³ ë‹¤ìŒ Layerë¡œ ì „ë‹¬ë¨
- ì—¬ëŸ¬ ì¸µì„ ìŒ“ìœ¼ë©´ ë” ë³µìž¡í•œ í•¨ìˆ˜/ê³µê°„ êµ¬ì¡°ë„ ê·¼ì‚¬ ê°€ëŠ¥

---

## ðŸ”¬ [ì‹¤ìŠµ] ë„˜íŒŒì´ vs íŒŒì´í† ì¹˜ë¡œ 2ë‹¨ MLP ìˆœì „íŒŒ(Forward) ë¹„êµ

### 1ï¸âƒ£ ë„˜íŒŒì´ë¡œ êµ¬í˜„

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# ìž„ì˜ íŒŒë¼ë¯¸í„°/ìž…ë ¥
W1 = np.array([[0.1, 0.2], [0.3, 0.4]])
b1 = np.array([0.1, 0.2])
W2 = np.array([[0.5], [0.6]])
b2 = np.array([0.3])

x = np.array([1.0, 0.5])

# Forward ì—°ì‚°
z1 = np.dot(x, W1) + b1
a1 = sigmoid(z1)
z2 = np.dot(a1, W2) + b2
a2 = sigmoid(z2)

print("NumPy ìµœì¢… ì¶œë ¥:", a2)
```

---

### 2ï¸âƒ£ íŒŒì´í† ì¹˜ë¡œ êµ¬í˜„

```python
import torch
import torch.nn as nn

# ë™ì¼ íŒŒë¼ë¯¸í„°/ìž…ë ¥
W1 = torch.tensor([[0.1, 0.2], [0.3, 0.4]], dtype=torch.float32)
b1 = torch.tensor([0.1, 0.2], dtype=torch.float32)
W2 = torch.tensor([[0.5], [0.6]], dtype=torch.float32)
b2 = torch.tensor([0.3], dtype=torch.float32)
x = torch.tensor([1.0, 0.5], dtype=torch.float32)

def sigmoid(x):
    return 1 / (1 + torch.exp(-x))

# Forward ì—°ì‚°
z1 = torch.matmul(x, W1) + b1
a1 = sigmoid(z1)
z2 = torch.matmul(a1, W2) + b2
a2 = sigmoid(z2)

print("PyTorch ìµœì¢… ì¶œë ¥:", a2.item())
```

---

### âœ… [ì‹¤í—˜ ê²°ê³¼]
- ê°™ì€ íŒŒë¼ë¯¸í„°, ê°™ì€ ì—°ì‚°ì‹ì´ë©´ **NumPyì™€ PyTorch ëª¨ë‘ ìˆœì „íŒŒ(Forward) ê²°ê³¼ê°€ ë™ì¼**í•¨ì„ ì§ì ‘ í™•ì¸  
- ì‹ ê²½ë§ êµ¬ì¡°ê°€ ê°™ìœ¼ë©´ í”„ë ˆìž„ì›Œí¬ì— ìƒê´€ì—†ì´ ì—°ì‚° ê²°ê³¼ë„ ê°™ë‹¤

---

## í”„ë¡œì íŠ¸: XOR ë¬¸ì œ í•™ìŠµ

```python
import torch
import torch.nn as nn
import torch.optim as optim

x = torch.tensor([[0.,0.],[0.,1.],[1.,0.],[1.,1.]])
y = torch.tensor([[0.],[1.],[1.],[0.]])

model = nn.Sequential(
    nn.Linear(2, 4),
    nn.Sigmoid(),
    nn.Linear(4, 1),
    nn.Sigmoid()
)

criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)

for epoch in range(10000):
    y_pred = model(x)
    loss = criterion(y_pred, y)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if epoch % 1000 == 0:
        print(f"Epoch {epoch:5d}: loss = {loss.item():.4f}")

print(model(x).detach())
```

---

## ðŸ”¹ ê²°ê³¼ ë³´ê³ 
- XOR ë¬¸ì œëŠ” ë‹¨ì¼ í¼ì…‰íŠ¸ë¡ ìœ¼ë¡œëŠ” êµ¬í˜„ ë¶ˆê°€ëŠ¥
- ë‹¤ì¸µ êµ¬ì¡°ë¥¼ ì ìš©í•˜ë©´ êµ¬í˜„ ê°€ëŠ¥

---

## ðŸ”§ ê³¼ì œ

1. ìœ„ ì‹¤ìŠµ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê³  íŒŒë¼ë¯¸í„°(W, b)ì™€ ìž…ë ¥ê°’ì„ ë°”ê¿”ê°€ë©° ê²°ê³¼ ë¹„êµ  
2. ì‹œê·¸ëª¨ì´ë“œ ëŒ€ì‹  ë‹¤ë¥¸ í™œì„±í™” í•¨ìˆ˜(ReLU ë“±)ë¡œ ì‹¤í—˜  
3. 3ë‹¨ ì´ìƒ MLP êµ¬ì¡°ë¡œ í™•ìž¥  
4. (ì‹¬í™”) 4ì£¼ì°¨ì—ì„œ ì—­ì „íŒŒ(gradient)ê¹Œì§€ íŒŒì´í† ì¹˜/ë„˜íŒŒì´ë¡œ ë¹„êµí•´ë³´ê¸°  
5. XOR ë¬¸ì œ í”„ë¡œì íŠ¸ ìž‘ì„± í›„, lossê°€ 0.01 ì´í•˜ê°€ ë˜ë„ë¡ í•™ìŠµ, ì¶œë ¥ê°’ê³¼ íŒŒë¼ë¯¸í„° ê¸°ë¡

---

âœ… **í•„ìš” í™˜ê²½**: Python 3.x, Google Colab, numpy, torch >= 2.0  
ì„¤ì¹˜: `!pip install numpy torch torchvision -q`
