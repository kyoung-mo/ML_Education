
# ğŸ“˜ Week3â€‘2 â€“ ì†ì‹¤ í•¨ìˆ˜ & ì—­ì „íŒŒ (yâ€‘hat ì„¤ëª… í¬í•¨)
---

## ğŸ“‰ ì†ì‹¤ í•¨ìˆ˜ *(Loss Function)*

### ğŸ” `y`Â·`Å·`(yâ€‘hat) ê¸°í˜¸ ì´í•´
| ê¸°í˜¸ | ì½ëŠ” ë²• | ì˜ë¯¸ |
|------|--------|------|
| **`y`** | ì™€ì´ | **ì‹¤ì œê°’**Â·ì •ë‹µ(Ground Truth) |
| **`Å·`** (yâ€‘hat) | ì™€ì´â€‘í–‡ | **ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê°’**Â·ì¶”ì •ì¹˜(Prediction) |

> ëª¨ì(^) ëª¨ì–‘ **â€œhatâ€** ì€ í†µê³„Â·ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ â€œì¶”ì •(estimation)â€ì„ ë‚˜íƒ€ë‚´ëŠ” ê´€ë¡€ì  í‘œê¸°ì…ë‹ˆë‹¤.  
> ì¦‰, ì†ì‹¤ í•¨ìˆ˜ëŠ” **ì‹¤ì œê°’ `y`** ì™€ **ì˜ˆì¸¡ê°’ `Å·`** ì˜ ì°¨ì´ë¥¼ ìˆ˜ì¹˜í™”í•©ë‹ˆë‹¤.

---

### 1ï¸âƒ£ ì†ì‹¤ í•¨ìˆ˜ ëª¨ìŒ

| ë²”ì£¼ | í•¨ìˆ˜ | ì‹ (ìš”ì•½) | íŠ¹ì§• / ì‚¬ìš©ì²˜ |
|------|------|-----------|----------------|
| íšŒê·€ | **MSE** | $\dfrac{1}{N} \sum (y - \hat{y})^2$ | ì´ìƒì¹˜ ë¯¼ê°, ë³´í¸ì  |
|      | MAE | $\dfrac{1}{N} \sum \lvert y - \hat{y} \rvert$ | ì´ìƒì¹˜ ê°•ì¸ |
| ì´ì§„ ë¶„ë¥˜ | **BCE** | $- \big[ y \log \hat{y} + (1 - y) \log (1 - \hat{y}) \big]$ | ì‹œê·¸ëª¨ì´ë“œ ì´í›„ ì‚¬ìš© |
| ë‹¤ì¤‘ ë¶„ë¥˜ | **Cross-Entropy** | $- \sum y_k \log \hat{p}_k$ | ì†Œí”„íŠ¸ë§¥ìŠ¤ ì´í›„ |
| ë¶ˆê· í˜• | Focal Loss | $- (1 - \hat{p})^{\gamma} \log \hat{p}$ | ì–´ë ¤ìš´ ìƒ˜í”Œ ì§‘ì¤‘ |
| ë¶„í¬ ê±°ë¦¬ | KL Divergence | $\sum p \log \frac{p}{q}$ | ì§€ì‹ ì¦ë¥˜, VAE |

> ì—¬ê¸°ì„œ $\hat{p}$ ì—­ì‹œ **ì˜ˆì¸¡ í™•ë¥ **ì„ ëœ»í•©ë‹ˆë‹¤.



### 2ï¸âƒ£ PyTorch ì˜ˆì œ
```python
import torch
import torch.nn as nn

logits  = torch.tensor([[2.0, 0.5, -1.0]])  # ëª¨ë¸ì´ ë‚¸ ì ìˆ˜(logit)
targets = torch.tensor([0])                 # ì •ë‹µ ë ˆì´ë¸”

loss_fn = nn.CrossEntropyLoss()
loss = loss_fn(logits, targets)
print("Crossâ€‘Entropy Loss:", loss.item())
```
`CrossEntropyLoss` ë‚´ë¶€ì—ì„œ  
\( \hat{p} = \text{softmax}(\text{logits}) \) ë¥¼ ê³„ì‚°í•œ ë’¤ ì†ì‹¤ì„ ì¶œë ¥í•©ë‹ˆë‹¤.

---

## ğŸ” ì—­ì „íŒŒ *(Backpropagation)*

### 1ï¸âƒ£ ê°œë…
- ì†ì‹¤ì´ ì¤„ì–´ë“¤ë„ë¡ **íŒŒë¼ë¯¸í„°(ê°€ì¤‘ì¹˜Â·í¸í–¥)** ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜  
- **ì²´ì¸ ë£°** ë¡œ ë‹¤ì¸µ ë¯¸ë¶„ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°

### 2ï¸âƒ£ PyTorch í•™ìŠµ ì ˆì°¨

| ë‹¨ê³„ | ì½”ë“œ | ì—­í•  |
|------|------|------|
| ìˆœì „íŒŒ | `y_pred = model(x)` | ê·¸ë˜í”„ ìƒì„± |
| ì†ì‹¤ ê³„ì‚° | `loss = criterion(y_pred, y)` | ë…¸ë“œ ì¶”ê°€ |
| ì—­ì „íŒŒ | `loss.backward()` | âˆ‚L/âˆ‚Î¸ ê³„ì‚° |
| ì—…ë°ì´íŠ¸ | `optimizer.step()` | Î¸ â† Î¸Â âˆ’Â Î·Â·âˆ‚L/âˆ‚Î¸ |
| ì´ˆê¸°í™” | `optimizer.zero_grad()` | ê·¸ë˜ë””ì–¸íŠ¸ ë¦¬ì…‹ |

---

## ğŸ› ï¸ ì‹¤ìŠµ: ì„ í˜• íšŒê·€ë¡œ `y=3x` ê·¼ì‚¬
```python
import torch, torch.nn as nn, torch.optim as optim

model = nn.Linear(1, 1)                 # y = wx + b
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.05)

x = torch.tensor([[1.0], [2.0], [3.0]])
y = torch.tensor([[3.0], [6.0], [9.0]])  # y=3x

for epoch in range(200):
    pred = model(x)
    loss = criterion(pred, y)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch+1) % 20 == 0:
        print(f"Epoch {epoch+1:3d}: Loss={loss.item():.4f}")

print("í•™ìŠµëœ w, b:", model.weight.item(), model.bias.item())
```

---

## ğŸ¯ ê³¼ì œ

1. ìœ„ ì˜ˆì œë¥¼ ì°¸ê³ í•´ `nn.Linear(1,1)` ë¡œ **`y = 3x`** ê·¼ì‚¬í•˜ê¸°  
2. `nn.MSELoss()` ì‚¬ìš©, **ì—í­ë§ˆë‹¤ ì†ì‹¤** ì¶œë ¥  
3. í•™ìŠµ ì¢…ë£Œ í›„ **weight(â‰ˆ3) & bias(â‰ˆ0)** í™•ì¸

---

âœ… **í™˜ê²½**: PythonÂ 3.x, GoogleÂ Colab, PyTorch â‰¥Â 2.0  
ì„¤ì¹˜: `!pip install torch torchvision -q`
