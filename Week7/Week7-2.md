# ğŸ“˜ Week7-2 â€“ Transformer ì‹¬í™”: Self-Attention, Multi-Head, êµ¬í˜„

---

## âœ¨ Self-Attentionì´ë€?

- ê° ì…ë ¥(í† í°)ì´ ì „ì²´ ì‹œí€€ìŠ¤ì˜ ëª¨ë“  ì…ë ¥ê³¼ **ìƒí˜¸ì‘ìš©**í•˜ì—¬ â€œë¬¸ë§¥ì˜ ì¤‘ìš”ë„â€ë¥¼ ë™ì ìœ¼ë¡œ í•™ìŠµ
- RNNê³¼ ë‹¬ë¦¬ â€œí•œ ë²ˆì— ëª¨ë“  ìœ„ì¹˜ì˜ ê´€ê³„â€ë¥¼ ê³„ì‚°

### ìˆ˜ì‹ (Scaled Dot-Product Attention)
- ì…ë ¥: Query(Q), Key(K), Value(V) í–‰ë ¬  
- ê³„ì‚°:
  $$
  \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
  $$
  - $d_k$: Kì˜ ì°¨ì›ìˆ˜ (ìŠ¤ì¼€ì¼ë§, ì•ˆì •ì„±â†‘)

---

## ğŸ”¢ Self-Attention ì—°ì‚° íë¦„

1. **Query/Key/Value** í–‰ë ¬ ê³„ì‚° (ì„ í˜• ë³€í™˜)
2. $QK^T$ (ìœ ì‚¬ë„ ì ìˆ˜) â†’ softmaxë¡œ â€œì¤‘ìš”ë„â€ ê²°ì •
3. ê° ìœ„ì¹˜ì˜ Valueë¥¼ ì¤‘ìš”ë„ë§Œí¼ í•©ì‚° (ê°€ì¤‘í•©)

|  | ì…ë ¥ ë¬¸ì¥ | Q/K/V ìƒì„± | ìœ ì‚¬ë„ | ê°€ì¤‘í•© | ì¶œë ¥ |
|--|----------|-----------|--------|-------|------|
| ex | I like pizza | Q, K, V | Iâ†”pizza | pizza ê°•ì¡° | |

---

## ğŸ§© Multi-Head Attention

- ì—¬ëŸ¬ ê°œì˜ Self-Attentionì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰, ì„œë¡œ ë‹¤ë¥¸ â€œê´€ì (ì„œë¸ŒìŠ¤í˜ì´ìŠ¤)â€ì—ì„œ ë¬¸ë§¥ íŒŒì•…  
- ë‹¤ì–‘í•œ ê´€ê³„ë¥¼ ë™ì‹œì— í•™ìŠµ ê°€ëŠ¥

### ê·¸ë¦¼

![multi-head attention](https://jalammar.github.io/images/t/multi_head_attention.png)

---

## ğŸ”§ Feed Forward, LayerNorm, Residual

- **Feed Forward Network (FFN)**: ê° í† í°ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ë˜ëŠ” MLP(2ê°œ Linear + í™œì„±í™”)
- **Layer Normalization**: ê° ë ˆì´ì–´ì˜ ë¶„ì‚°/í‰ê·  ì •ê·œí™” (í•™ìŠµ ì•ˆì •í™”)
- **Residual Connection**: ì…ë ¥+ì¶œë ¥ ë”í•´ gradient íë¦„ ê°œì„ , ì„±ëŠ¥ í–¥ìƒ

---

## ğŸ—‚ï¸ íŒŒì´í† ì¹˜ Transformer êµ¬í˜„ ì˜ˆì‹œ

```python
import torch
import torch.nn as nn

model = nn.Transformer(
    d_model=16, nhead=4, num_encoder_layers=2, num_decoder_layers=2,
    dim_feedforward=32, batch_first=True
)
src = torch.rand(2, 5, 16)  # (batch, seq, feature)
tgt = torch.rand(2, 5, 16)
out = model(src, tgt)
print(out.shape)  # (batch, seq, d_model)
```

---

## ğŸ› ï¸ ì‹¤ìŠµ ê³¼ì œ

1. nn.Transformerë¥¼ ì‚¬ìš©í•´, ì…ë ¥ ì°¨ì›/ë ˆì´ì–´/head ê°œìˆ˜ë¥¼ ë°”ê¿”ê°€ë©° ì‹¤í—˜  
2. ì§ì ‘ Multi-Head Self-Attention ë ˆì´ì–´ êµ¬í˜„ (íŒŒì´í† ì¹˜ ê³µì‹ë¬¸ì„œ ì°¸ê³ )
3. FeedForward/LayerNorm/Residual ì—­í•  ì‹¤í—˜ (ì¶”ê°€/ì‚­ì œ í›„ ì„±ëŠ¥ ë¹„êµ)
4. ì‹¤ì œ ë…¼ë¬¸/ì‹¤ì œ ëª¨ë¸(GPT, BERT ë“±)ì—ì„œ Attention ê²°ê³¼ ì‹œê°í™” ìë£Œ ì¡°ì‚¬

---

## ğŸ“š ì°¸ê³ : Transformer ì‘ìš©/íŒŒìƒ

- **BERT**: Encoderë§Œ, ë¬¸ì¥ ì´í•´/ë¶„ë¥˜/ì§ˆë¬¸ì‘ë‹µ
- **GPT**: Decoderë§Œ, í…ìŠ¤íŠ¸ ìƒì„±
- **ViT**: ì´ë¯¸ì§€ íŒ¨ì¹˜ ì…ë ¥, ë¹„ì „ ì˜ì—­
- **LLM/ë©€í‹°ëª¨ë‹¬**: ëŒ€ê·œëª¨ íŒŒë¼ë¯¸í„°, ì–¸ì–´+ì´ë¯¸ì§€ ë“± ë‹¤ì–‘í•œ ì…ë ¥

---

âœ… **í™˜ê²½**: Python 3.x, Google Colab, PyTorch â‰¥ 2.0  
ì„¤ì¹˜: `!pip install torch torchvision -q`
