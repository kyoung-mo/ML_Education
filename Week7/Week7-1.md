# ğŸ“˜ Week7-1 â€“ Transformer: ì´ë¡ ê³¼ êµ¬ì¡°

---

## ğŸ” Transformerë€?

- 2017ë…„ êµ¬ê¸€ì´ ë°œí‘œí•œ â€œAttention is All You Needâ€ ë…¼ë¬¸ì—ì„œ ì²˜ìŒ ì œì•ˆëœ **ë”¥ëŸ¬ë‹ ì‹œí€€ìŠ¤ ëª¨ë¸**ì…ë‹ˆë‹¤.
- RNN/GRU/LSTM ì—†ì´ë„ **ëª¨ë“  ì‹œí€€ìŠ¤ ì •ë³´ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬**í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ëœ êµ¬ì¡°.
- ëŒ€í‘œì  íŠ¹ì§•: **Self-Attention**(ìê¸°ì–´í…ì…˜), **ë³‘ë ¬ ì²˜ë¦¬**, **ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ í•´ê²°**
- ìì—°ì–´ì²˜ë¦¬(NLP), ë¹„ì „, ë©€í‹°ëª¨ë‹¬ ë“± ê´‘ë²”ìœ„í•˜ê²Œ ì‚¬ìš©

---

## ğŸ¤” ì™œ Transformerê°€ í•„ìš”í• ê¹Œ?

| RNN/LSTM/GRU | Transformer |
|--------------|-------------|
| ì‹œí€€ìŠ¤ ë°ì´í„° ì•/ë’¤ë¡œ ìˆœì°¨ì²˜ë¦¬ | í•œ ë²ˆì— ì „ì²´ ì…ë ¥ ì²˜ë¦¬ (ë³‘ë ¬í™”) |
| ê¸´ ì˜ì¡´ê´€ê³„ í•™ìŠµ ì–´ë ¤ì›€(ê¸°ìš¸ê¸° ì†Œì‹¤) | ë©€ë¦¬ ë–¨ì–´ì§„ í† í°ë„ ì‰½ê²Œ ì—°ê²° |
| ê³„ì‚°ì†ë„ ëŠë¦¼ (ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë¹„ë¡€) | ë¹ ë¥¸ ì—°ì‚°(ë³‘ë ¬)ê³¼ í™•ì¥ì„± |

- **ì¥ì :** ê¸´ ë¬¸ì¥, ë¬¸ë§¥, ë³µì¡í•œ ì˜ì¡´ì„±, ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥  
- **í˜ì‹ :** GPT, BERT, ViT ë“± ê±°ì˜ ëª¨ë“  ìµœì‹  ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ê¸°ë°˜

---

## ğŸ§© Transformerì˜ ê¸°ë³¸ êµ¬ì¡°

- **ì…ë ¥ ì„ë² ë”©(Input Embedding)**
- **í¬ì§€ì…”ë„ ì¸ì½”ë”©(Positional Encoding)**
- **ì¸ì½”ë”(Encoder) ë¸”ë¡**
- **ë””ì½”ë”(Decoder) ë¸”ë¡**  
- **ì¶œë ¥(ì˜ˆ: ë‹¨ì–´, í† í° ë“±)**

> êµ¬ì¡°ì˜ í•µì‹¬: **Self-Attention**ê³¼ **Feedforward Network**  
> ê° ë¸”ë¡ì€ Layer Normalization, ì”ì°¨ì—°ê²°(Residual Connection), Dropout ë“±ìœ¼ë¡œ êµ¬ì„±

---

### ì „ì²´ ì•„í‚¤í…ì²˜ ê·¸ë¦¼

![Transformer êµ¬ì¡°](https://jalammar.github.io/images/t/transformer_architecture.png)

ì¶œì²˜: [jalammar.github.io/illustrated-transformer](https://jalammar.github.io/illustrated-transformer/)

---

### **Encoder/Decoder ìš”ì•½**

| Encoder                   | Decoder                         |
|---------------------------|---------------------------------|
| ì…ë ¥ ì„ë² ë”© + í¬ì§€ì…”ë„ ì¸ì½”ë”© | ì…ë ¥ ì„ë² ë”© + í¬ì§€ì…”ë„ ì¸ì½”ë”©    |
| Self-Attention            | Self-Attention (Masked)         |
| Feedforward Layer         | Cross-Attention (ì¸ì½”ë”ì™€ ì—°ê²°)  |
| (NíšŒ ë°˜ë³µ)                | Feedforward Layer (NíšŒ ë°˜ë³µ)    |

- **Self-Attention**: ê° í† í°ì´ ë¬¸ë§¥ ë‚´ ë‹¤ë¥¸ í† í°ê³¼ ê´€ê³„ë¥¼ ë™ì ìœ¼ë¡œ íŒŒì•…  
- **Cross-Attention**: ë””ì½”ë”ê°€ ì¸ì½”ë”ì˜ ì¶œë ¥ê³¼ë„ ê´€ê³„ íŒŒì•…

---

## ğŸ”¢ í¬ì§€ì…”ë„ ì¸ì½”ë”© (Positional Encoding)

- TransformerëŠ” ì…ë ¥ ìˆœì„œë¥¼ ì§ì ‘ ì¸ì‹í•˜ì§€ ëª»í•¨ â†’ **ìœ„ì¹˜ ì •ë³´ë¥¼ ì„ë² ë”©ì— ë”í•´ì¤Œ**
- ëŒ€í‘œì  ë°©ì‹(ë…¼ë¬¸ ê¸°ì¤€): ì‚¬ì¸/ì½”ì‚¬ì¸ í•¨ìˆ˜ë¥¼ ê° ì°¨ì›ë§ˆë‹¤ ë‹¤ë¥´ê²Œ ì‚¬ìš©

$$
PE_{(pos, 2i)} = \sin \left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$
$$
PE_{(pos, 2i+1)} = \cos \left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$

- $pos$: ìœ„ì¹˜, $i$: ì„ë² ë”© ë‚´ ì°¨ì› ì¸ë±ìŠ¤, $d_{model}$: ì„ë² ë”© ì°¨ì›ìˆ˜

---

### íŒŒì´í† ì¹˜ ì˜ˆì‹œ ì½”ë“œ

```python
import torch
import math

def positional_encoding(seq_len, d_model):
    pe = torch.zeros(seq_len, d_model)
    for pos in range(seq_len):
        for i in range(0, d_model, 2):
            pe[pos, i]   = math.sin(pos / (10000 ** ((2 * i)/d_model)))
            if i+1 < d_model:
                pe[pos, i+1] = math.cos(pos / (10000 ** ((2 * i)/d_model)))
    return pe

pe = positional_encoding(8, 16)
print(pe)
```

---

## ğŸ“ Week7-1 ì •ë¦¬

- TransformerëŠ” ìˆœí™˜ ì—†ì´ ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ ë™ì‹œì— ì²˜ë¦¬
- Self-Attention, Position Encoding ë“±ìœ¼ë¡œ ë¬¸ë§¥ê³¼ ìœ„ì¹˜ ì •ë³´ ëª¨ë‘ í™œìš©
- Encoder-Decoder êµ¬ì¡°ëŠ” ë²ˆì—­, ìš”ì•½, ìƒì„± ë“± ê´‘ë²”ìœ„í•˜ê²Œ ì ìš©

---

## ğŸ› ï¸ ì‹¤ìŠµ ê³¼ì œ

1. ìœ„ íŒŒì´í† ì¹˜ Positional Encoding ì½”ë“œ ì‹¤í–‰, ë‹¤ì–‘í•œ seq_len/d_modelë¡œ ì‹œê°í™”  
2. Encoder/Decoder ë¸”ë¡ì„ ê·¸ë¦¼ìœ¼ë¡œ ê·¸ë ¤ë³´ê³  ê° ë ˆì´ì–´ ì—­í•  ì •ë¦¬  
3. ë³¸ì¸ì˜ ì „ê³µ/ê´€ì‹¬ ë¶„ì•¼ì—ì„œ Transformer ì‘ìš© ê°€ëŠ¥ì„±ì„ ì¡°ì‚¬  
4. (ì‹¬í™”) ë…¼ë¬¸ ìš”ì•½ â€œAttention is All You Needâ€ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½

---

âœ… **í™˜ê²½**: Python 3.x, Google Colab, PyTorch â‰¥ 2.0  
ì„¤ì¹˜: `!pip install torch torchvision -q`
