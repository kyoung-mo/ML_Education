# ðŸ“˜ Week4-2 â€“ ì†ì‹¤í•¨ìˆ˜ & ì—­ì „íŒŒ

---

## ðŸ” â€œê°’ì˜ ì°¨â€ë¥¼ ë§í•˜ëŠ” ê²ƒ: Loss Function

### â–¶ï¸ ê¸°í˜¸ ì •ë³´
| ê¸°í˜¸ | ì½ëŠ” ë°©ë²• | ì˜ë¯¸ |
|--------|-------------|------|
| `y` | ì‹¤ì œê°’ (Ground Truth) | ì •ë³´, ê²°ê³¼ |
| `Å·` (y-hat) | ì˜ˆì¸¡ê°’ (Prediction) | ëª¨ë¸ì˜ ì¶œë ¥ |

> â€œê°’ê³¼ ê°’ì˜ ì°¨â€ì™€ ê°™ì€ ë‹¨ì–´ê°€ ë§¤ìš° ìžì„¸í•œ ê²ƒê³¼ ê°™ì§€ë§Œ, ì—¬ê¸°ì„œê³  ë°”ë¡œ **Loss**ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤.

---

## ðŸ”¢ ì†ì‹¤í•¨ìˆ˜(Loss Function) ë¶„ë¥˜

| ë²”ì£¼ | í•¨ìˆ˜ | ì‹ | íŠ¹ì§• / ì‚¬ìš©ì²˜ |
|------|------|----------|----------------|
| íšŒê·€ | MSE | $\dfrac{1}{N} \sum (y - \hat{y})^2$ | ë³µìˆ˜ê°€ í° ê²½ìš° ë¬´ê°€ëŠ¥ |
|        | MAE | $\dfrac{1}{N} \sum \lvert y - \hat{y} 
vert$ | ì´ìƒì¹˜ ê°•ì¸ |
| ì´ì§€ë¥´ ë¶„ë¥˜ | BCE | $- [y \log \hat{y} + (1 - y) \log (1 - \hat{y})]$ | ì‹œê·¸ëª¬ë“œ í›„ ì‚¬ìš© |
| ë‹¤ì¤‘ ë¶„ë¥˜ | Cross-Entropy | $-\sum y_k \log \hat{p}_k$ | ì†Œí”„íŠ¸ë§¥ìŠ¤ í›„ ì‚¬ìš© |
| ë¶ˆê·œí¬ | Focal Loss | $-(1 - \hat{p})^{\gamma} \log \hat{p}$ | ì–´ëŠ ì‚¬ëžŒì´ ë” ì–´ë¦°ì§€ ì¡°ì • |
| ë¶„í¬ ê±°ë¦¬ | KL Divergence | $\sum p \log \dfrac{p}{q}$ | ì§€ì‹œ ì¦ë¥˜, VAE |

---

## ðŸ’¡ PyTorch ì˜ˆì œ: CrossEntropyLoss

```python
import torch
import torch.nn as nn

logits  = torch.tensor([[2.0, 0.5, -1.0]])  # ëª¨ë¸ì˜ ì¶”ë¦¬
labels = torch.tensor([0])                # ì •ë³´

loss_fn = nn.CrossEntropyLoss()
loss = loss_fn(logits, labels)
print("Cross-Entropy Loss:", loss.item())
```
> `nn.CrossEntropyLoss()`ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ `softmax(logits)`ë¥¼ ê³„ì‚°í•´ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ëŠ” íŒ¨í„´ìž…ë‹ˆë‹¤.

---

## ðŸ”„ ì—­ì „íŒŒ *(Backpropagation)*

### â–¶ï¸ ê¸°ë³¸ ê°œë…
- Lossê°€ ìµœì†Œí™”ë˜ë„ë¡ **íŒŒë¼ë¯¸í„°** (weights, bias)ë¥¼ ë³€ê²½
- ê·¸ëž˜í”„ì˜ ê²°ê³¼ë¥¼ í†µí•´ ì—­ì „ì ìœ¼ë¡œ ë°˜ì˜
- ë°˜ë³µë˜ëŠ” ì–‘ìˆ˜ì  ê³„ì‚°ì„ **Chain Rule**ë¡œ ì°¨ê°€í•˜ê²Œ ê³„ì‚°

### â–¶ï¸ PyTorch ê²½ìš°

| ë‹¨ê³„ | ì½”ë“œ | ì—­í•  |
|------|--------|------|
| ìˆœì „íŒŒ | `y_pred = model(x)` | ê·¸ëž˜í”„ ìƒì„± |
| Loss ê³„ì‚° | `loss = criterion(y_pred, y)` | ë…¸ë“œ ì¶”ê°€ |
| ì—­ì „íŒŒ | `loss.backward()` | \( \partial L/\partial 	heta \) ê³„ì‚° |
| ê°±ì‹  | `optimizer.step()` | \( 	heta \leftarrow 	heta - \eta \cdot \partial L/\partial 	heta \) |
| ê·¸ëž˜ë””ì–¸íŠ¸ ë¦¬ì…‹ | `optimizer.zero_grad()` | ê³„ì‚° ì²˜ìŒí™” |

---

## ðŸ”§ ì‹¤ìŠµ: y = 3x ê·¸ë¦¬ê¸°

```python
import torch, torch.nn as nn, torch.optim as optim

model = nn.Linear(1, 1)                 # y = wx + b
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.05)

x = torch.tensor([[1.0], [2.0], [3.0]])
y = torch.tensor([[3.0], [6.0], [9.0]])  # y=3x

for epoch in range(200):
    pred = model(x)
    loss = criterion(pred, y)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch+1) % 20 == 0:
        print(f"Epoch {epoch+1:3d}: Loss={loss.item():.4f}")

print("
í•™ìŠµëœ w, b:", model.weight.item(), model.bias.item())
```

---

## ðŸŽ¯ ê³¼ì œ

1. `nn.Linear(1,1)`ë¡œ y=3x ê·¸ë¦¬ê¸°
2. `nn.MSELoss()` ì‚¬ìš©, ê²½ê³¼ì— ì¡°ëª… í‘œì‹œ
3. íŒŒë¼ë¯¸í„° (weight ê·¸ë£¹) ê°’ ê²€ìƒ‰

---

âœ… í•„ìš” í™˜ê²½: Python 3.x, Google Colab, PyTorch >= 2.0  
> ì„¤ì¹˜: `!pip install torch torchvision -q`
