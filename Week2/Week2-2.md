
# ğŸ“˜ Week2â€‘2 â€“ scikitâ€‘learn, ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë³¸ ê°œë…
---

## ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë³¸ ê°œë…

### ğŸ”¹ ë¨¸ì‹ ëŸ¬ë‹ì´ë€?
- ëª…ì‹œì  ê·œì¹™(rule)ì„ ì½”ë”©í•˜ëŠ” ëŒ€ì‹ , **ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµ**í•˜ì—¬ ìƒˆë¡œìš´ ì…ë ¥ì„ ì˜ˆì¸¡Â·ë¶„ë¥˜í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.  
- í¬ê²Œ **ì§€ë„í•™ìŠµ(Supervised)**, **ë¹„ì§€ë„í•™ìŠµ(Unsupervised)**, **ê°•í™”í•™ìŠµ(Reinforcement)** ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.

| í•™ìŠµ ìœ í˜• | ì…ë ¥ | ëª©í‘œ | ëŒ€í‘œ ì•Œê³ ë¦¬ì¦˜ |
|-----------|------|------|--------------|
| ì§€ë„í•™ìŠµ | íŠ¹ì§• + ì •ë‹µ | ë³´ì´ì§€ ì•ŠëŠ” ë°ì´í„°ì˜ **ì •ë‹µ ì˜ˆì¸¡** | Linear/Logistic Regression, SVM, Random Forest |
| ë¹„ì§€ë„í•™ìŠµ | íŠ¹ì§•ë§Œ | **êµ¬ì¡° ë°œê²¬Â·êµ°ì§‘Â·ì°¨ì›ì¶•ì†Œ** | Kâ€‘Means, PCA, AutoEncoder |
| ê°•í™”í•™ìŠµ | ìƒíƒœ + ë³´ìƒ | **í–‰ë™ ì •ì±… ìµœì í™”** | Qâ€‘Learning, Policy Gradient |

### ğŸ“Š ì „ì²´ íë¦„ë„

```
[ë°ì´í„° ìˆ˜ì§‘]
      â†“
[ì „ì²˜ë¦¬ ë° íŠ¹ì§•/ë ˆì´ë¸” ë¶„ë¦¬]
      â†“
[í•™ìŠµ ë°ì´í„° / í‰ê°€ ë°ì´í„° ë‚˜ëˆ„ê¸°]
      â†“
[ëª¨ë¸ ì„ íƒ ë° í•™ìŠµ]
      â†“
[ëª¨ë¸ í‰ê°€]
      â†“
[ìƒˆë¡œìš´ ì…ë ¥ ë°ì´í„° ì˜ˆì¸¡]
```

### ğŸ“˜ ì£¼ìš” ìš©ì–´

| ìš©ì–´ | ì˜ë¯¸ | ì˜ˆì‹œ |
|------|------|------|
| **íŠ¹ì§• (feature)** | ì…ë ¥ ë°ì´í„°ì˜ ì†ì„± ê°’ | í‚¤, ëª¸ë¬´ê²Œ, ì„±ë³„, ë‚˜ì´ |
| **ë ˆì´ë¸” (label)** | ì˜ˆì¸¡í•˜ë ¤ëŠ” ì •ë‹µ ê°’ | ë¹„ë§Œ ì—¬ë¶€ (ì˜ˆ: 0: ì •ìƒ, 1: ë¹„ë§Œ) |
| **ëª¨ë¸ (model)** | í•™ìŠµëœ ì˜ˆì¸¡ í•¨ìˆ˜ | "ì´ í‚¤ì™€ ëª¸ë¬´ê²Œë©´ ë¹„ë§Œì¼ í™•ë¥ ì€ 0.87" |
| **í›ˆë ¨ (training)** | ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ë³´ê³  íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ê³¼ì • | ì—¬ëŸ¬ ì‚¬ëŒì˜ í‚¤Â·ëª¸ë¬´ê²Œì™€ ë¹„ë§Œ ì—¬ë¶€ë¥¼ ì´ìš© |
| **í‰ê°€ (evaluation)** | ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì˜ ë§ì¶”ëŠ”ì§€ ì¸¡ì • | ìƒˆë¡œìš´ ì‚¬ëŒì˜ ë¹„ë§Œ ì—¬ë¶€ë¥¼ í…ŒìŠ¤íŠ¸ |
| **ê³¼ì í•© (overfitting)** | í›ˆë ¨ ë°ì´í„°ì—ë§Œ ë„ˆë¬´ ë§ì¶¤ | ìƒˆ ë°ì´í„°ì—ì„  ì˜¤ë‹µì´ ë§ì•„ì§ |
| **ì¼ë°˜í™” (generalization)** | ìƒˆ ë°ì´í„°ì—ì„œë„ ì˜ ì‘ë™í•˜ëŠ” ëŠ¥ë ¥ | ìƒˆë¡œìš´ ê²½ìš°ì—ë„ ë†’ì€ ì •í™•ë„ ìœ ì§€ |


> **Tip**Â   
> ê³¼ì í•©ì„ ë°©ì§€í•˜ë ¤ë©´ **ë°ì´í„° ë¶„í• , êµì°¨ê²€ì¦, ê·œì œ(regularization)** ë¥¼ ì ê·¹ í™œìš©í•˜ì„¸ìš”!

---

## ğŸ”§ scikitâ€‘learn ì†Œê°œ

### ğŸ”¹ scikitâ€‘learnì´ë€?
- NumPy/SciPy ê¸°ë°˜ **ë²”ìš© ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬**  
- **ì¼ê´€ëœ API**(`fit()`, `predict()`, `score()`)ì™€ **í’ë¶€í•œ ì „ì²˜ë¦¬Â·ëª¨ë¸Â·í‰ê°€** ë„êµ¬ ì œê³µ  
- ëŒ€ê·œëª¨ ë”¥ëŸ¬ë‹ë³´ë‹¤ëŠ” **í´ë˜ì‹ ML**ì— ìµœì í™” (CPU, ì¤‘ì†Œê·œëª¨ ë°ì´í„°ì…‹)

```python
import sklearn
print("scikit-learn version:", sklearn.__version__)
```

### ğŸ”¹ scikitâ€‘learn íŒŒì´í”„ë¼ì¸ êµ¬ì¡°
```
ë°ì´í„° âœ ì „ì²˜ë¦¬(Scaler/Encoder) âœ ëª¨ë¸ í•™ìŠµ âœ ì˜ˆì¸¡ âœ í‰ê°€
```
- **`Pipeline`** í´ë˜ìŠ¤ë¡œ ì ˆì°¨ë¥¼ **ì²´ì¸**í™”í•˜ë©´, ë°ì´í„° ëˆ„ì„¤(data leakage)ì„ ë°©ì§€í•˜ê³  í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ì„ ìë™í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ”§ ì „ì²˜ë¦¬(Preprocessing)ë€?

ì „ì²˜ë¦¬ëŠ” **ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì— ë°ì´í„°ë¥¼ ë„£ê¸° ì „ì—, ë°ì´í„°ë¥¼ ì •ë¦¬í•˜ê³  ê°€ê³µí•˜ëŠ” ê³¼ì •**ì…ë‹ˆë‹¤.  
ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì˜ ì´í•´í•˜ê³  í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” ë§¤ìš° ì¤‘ìš”í•œ ë‹¨ê³„ì…ë‹ˆë‹¤.

---

## âœ… ì „ì²˜ë¦¬ê°€ í•„ìš”í•œ ì´ìœ 

- í˜„ì‹¤ì˜ ë°ì´í„°ëŠ” ì¢…ì¢… **ê²°ì¸¡ì¹˜, ì´ìƒì¹˜, ë¬¸ìì—´, í¬ê¸° ì°¨ì´** ë“±ì´ í¬í•¨ë˜ì–´ ìˆì–´ ê·¸ëŒ€ë¡œëŠ” í•™ìŠµì´ ì–´ë µìŠµë‹ˆë‹¤.
- ì „ì²˜ë¦¬ë¥¼ í†µí•´ **ë°ì´í„°ë¥¼ ìˆ«ìí™”í•˜ê³ , ì •ë¦¬í•˜ê³ , ìŠ¤ì¼€ì¼ì„ ë§ì¶¤**ìœ¼ë¡œì¨ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ’¡ ì£¼ìš” ì „ì²˜ë¦¬ ì‘ì—…

- **ê²°ì¸¡ê°’ ì²˜ë¦¬**: ë¹ ì§„ ê°’ì„ í‰ê·  ë“±ìœ¼ë¡œ ì±„ìš°ê±°ë‚˜ ì œê±°
- **ì´ìƒì¹˜ ì œê±°**: ë„ˆë¬´ í°/ì‘ì€ ê°’ ì œê±°
- **ì¸ì½”ë”©**: ë¬¸ì ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë³€í™˜
- **ì •ê·œí™”/í‘œì¤€í™”**: ê°’ì˜ ë²”ìœ„ë¥¼ ì¼ì •í•˜ê²Œ ë§ì¶¤
- **ì¤‘ë³µ ì œê±° ë° íƒ€ì… ë³€í™˜** ë“±
---

## ğŸ› ï¸ ì‹¤ìŠµ: ë¶“ê½ƒ(Iris) ë¶„ë¥˜

### ğŸ”¹ 1) ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
```python
from sklearn.datasets import load_iris
iris = load_iris(as_frame=True)  # DataFrame í˜•íƒœë¡œ ë°›ê¸°
X, y = iris.data, iris.target
```

### ğŸ”¹ 2) í›ˆë ¨ / í…ŒìŠ¤íŠ¸ ë¶„ë¦¬
```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
```

### ğŸ”¹ 3) ì „ì²˜ë¦¬ + ëª¨ë¸ íŒŒì´í”„ë¼ì¸
```python
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline

pipe = Pipeline([
    ("scaler", StandardScaler()),
    ("knn",    KNeighborsClassifier(n_neighbors=3))
])

pipe.fit(X_train, y_train)
```

### ğŸ”¹ 4) ì„±ëŠ¥ í‰ê°€
```python
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
y_pred = pipe.predict(X_test)

print("ì •í™•ë„:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
```

---

## ğŸ” í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

`GridSearchCV`ë¥¼ í™œìš©í•˜ë©´ `n_neighbors`, `metric` ë“± íŒŒë¼ë¯¸í„°ë¥¼ **êµì°¨ê²€ì¦**ìœ¼ë¡œ ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from sklearn.model_selection import GridSearchCV

param_grid = {"knn__n_neighbors": [1, 3, 5, 7]}
grid = GridSearchCV(pipe, param_grid, cv=5)
grid.fit(X_train, y_train)

print("ìµœì  íŒŒë¼ë¯¸í„°:", grid.best_params_)
print("ê²€ì¦ ì •í™•ë„:", grid.best_score_)
```

---

## ğŸ§ª ê³¼ì œ

> **ë°ì´í„°ì…‹**: `load_wine()` (ì™€ì¸ í’ˆì§ˆ ë°ì´í„°)

1. ì™€ì¸ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³ , KNN ë¶„ë¥˜ê¸°ë¡œ **ê¸°ë³¸ ì •í™•ë„**ë¥¼ ì¸¡ì •í•˜ì„¸ìš”.  
2. `test_size`ë¥¼ **0.3**ìœ¼ë¡œ ë³€ê²½í•´ ì •í™•ë„ ë³€í™”ë¥¼ ë¹„êµí•˜ì„¸ìš”.  
3. `n_neighbors`ë¥¼ **1, 3, 5**ë¡œ ë°”ê¾¸ì–´ ê°ê° ì •í™•ë„ë¥¼ ê¸°ë¡í•˜ì„¸ìš”.

### ğŸ”‘ íŒíŠ¸
```python
from sklearn.datasets import load_wine
from sklearn.metrics import accuracy_score

wine = load_wine(as_frame=True)
X_train, X_test, y_train, y_test = train_test_split(
    wine.data, wine.target,
    test_size=0.3, stratify=wine.target, random_state=42
)

for k in [1, 3, 5]:
    model = KNeighborsClassifier(n_neighbors=k)
    model.fit(X_train, y_train)
    acc = model.score(X_test, y_test)
    print(f"k={k}, ì •í™•ë„={acc:.3f}")
```
> **ì¶”ê°€ ë„ì „ ğŸŒŸ**Â   
> â€¢ `StandardScaler`ë¥¼ íŒŒì´í”„ë¼ì¸ì— ë„£ì–´ ì„±ëŠ¥ì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ì‚´í´ë³´ì„¸ìš”.  
> â€¢ `cross_val_score`ë¡œ ì—¬ëŸ¬ í´ë“œ í‰ê·  ì •í™•ë„ë¥¼ êµ¬í•´ë³´ì„¸ìš”.

---

## ğŸ› ï¸ scikitâ€‘learn í€µ ë ˆí¼ëŸ°ìŠ¤

| ë²”ì£¼ | ëŒ€í‘œ í´ë˜ìŠ¤ | ì„¤ëª… |
|------|-------------|------|
| ì „ì²˜ë¦¬ | `StandardScaler`, `OneHotEncoder`, `PolynomialFeatures` | ì •ê·œí™”Â·ì¸ì½”ë”©Â·íŠ¹ì„± ìƒì„± |
| ë¶„ë¥˜ | `KNeighborsClassifier`, `SVC`, `RandomForestClassifier`, `LogisticRegression` | |
| íšŒê·€ | `LinearRegression`, `SVR`, `RandomForestRegressor` | |
| í´ëŸ¬ìŠ¤í„°ë§ | `KMeans`, `DBSCAN`, `AgglomerativeClustering` | ë¹„ì§€ë„ êµ°ì§‘ |
| ì°¨ì›ì¶•ì†Œ | `PCA`, `TSNE`, `TruncatedSVD` | |
| ëª¨ë¸ ì„ íƒ | `train_test_split`, `GridSearchCV`, `cross_val_score` | ë°ì´í„° ë¶„í• Â·CVÂ·íŠœë‹ |
| í‰ê°€ ì§€í‘œ | `accuracy_score`, `mean_squared_error`, `roc_auc_score` | |

---

âœ… **ì‚¬ìš© í™˜ê²½**: PythonÂ 3.x, GoogleÂ Colab (scikitâ€‘learn ê¸°ë³¸ íƒ‘ì¬)  
í•„ìš” ì‹œ `!pip install scikit-learn -U` ë¡œ ìµœì‹  ë²„ì „ì„ ì„¤ì¹˜í•˜ì„¸ìš”.
