
# ğŸ“˜ Week2â€‘2 â€“ scikitâ€‘learn, ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë³¸ ê°œë…
---

## ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë³¸ ê°œë…

### ğŸ”¹ ë¨¸ì‹ ëŸ¬ë‹ì´ë€?
- ëª…ì‹œì  ê·œì¹™(rule)ì„ ì½”ë”©í•˜ëŠ” ëŒ€ì‹ , **ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµ**í•˜ì—¬ ìƒˆë¡œìš´ ì…ë ¥ì„ ì˜ˆì¸¡Â·ë¶„ë¥˜í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.  
- í¬ê²Œ **ì§€ë„í•™ìŠµ(Supervised)**, **ë¹„ì§€ë„í•™ìŠµ(Unsupervised)**, **ê°•í™”í•™ìŠµ(Reinforcement)** ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.

| í•™ìŠµ ìœ í˜• | ì…ë ¥ | ëª©í‘œ | ëŒ€í‘œ ì•Œê³ ë¦¬ì¦˜ |
|-----------|------|------|--------------|
| ì§€ë„í•™ìŠµ | íŠ¹ì§• + ì •ë‹µ | ë³´ì´ì§€ ì•ŠëŠ” ë°ì´í„°ì˜ **ì •ë‹µ ì˜ˆì¸¡** | Linear/Logistic Regression, SVM, Random Forest |
| ë¹„ì§€ë„í•™ìŠµ | íŠ¹ì§•ë§Œ | **êµ¬ì¡° ë°œê²¬Â·êµ°ì§‘Â·ì°¨ì›ì¶•ì†Œ** | Kâ€‘Means, PCA, AutoEncoder |
| ê°•í™”í•™ìŠµ | ìƒíƒœ + ë³´ìƒ | **í–‰ë™ ì •ì±… ìµœì í™”** | Qâ€‘Learning, Policy Gradient |

### ğŸ”¹ ìš©ì–´ ì •ë¦¬
| ìš©ì–´ | ì˜ë¯¸ |
|------|------|
| **íŠ¹ì§•(feature)** | ì…ë ¥ ë°ì´í„°ì˜ ì†ì„± ê°’ (ì—´, ì»¬ëŸ¼) |
| **ë ˆì´ë¸”(label)** | ì˜ˆì¸¡í•˜ë ¤ëŠ” ê°’, ì •ë‹µ |
| **ëª¨ë¸(model)** | í•™ìŠµ ê²°ê³¼ë¬¼, ì˜ˆì¸¡ í•¨ìˆ˜ |
| **í›ˆë ¨(training)** | ëª¨ë¸ì´ íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµí•˜ëŠ” ê³¼ì • |
| **í‰ê°€(evaluation)** | í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ëª¨ë¸ í’ˆì§ˆ ì¸¡ì • |
| **ê³¼ì í•©(overfitting)** | í›ˆë ¨ ë°ì´í„°ì— ê³¼í•˜ê²Œ ì í•©í•´ ìƒˆ ë°ì´í„°ì—ì„œ ì„±ëŠ¥ ì €í•˜ |
| **ì¼ë°˜í™”(generalization)** | ë³´ì§€ ëª»í•œ ë°ì´í„°ì—ì„œë„ ì˜ ì‘ë™í•˜ëŠ” ëŠ¥ë ¥ |

> **Tip**Â   
> ê³¼ì í•©ì„ ë°©ì§€í•˜ë ¤ë©´ **ë°ì´í„° ë¶„í• , êµì°¨ê²€ì¦, ê·œì œ(regularization)** ë¥¼ ì ê·¹ í™œìš©í•˜ì„¸ìš”!

---

## ğŸ”§ scikitâ€‘learn ì†Œê°œ

### ğŸ”¹ scikitâ€‘learnì´ë€?
- NumPy/SciPy ê¸°ë°˜ **ë²”ìš© ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬**  
- **ì¼ê´€ëœ API**(`fit()`, `predict()`, `score()`)ì™€ **í’ë¶€í•œ ì „ì²˜ë¦¬Â·ëª¨ë¸Â·í‰ê°€** ë„êµ¬ ì œê³µ  
- ëŒ€ê·œëª¨ ë”¥ëŸ¬ë‹ë³´ë‹¤ëŠ” **í´ë˜ì‹ ML**ì— ìµœì í™” (CPU, ì¤‘ì†Œê·œëª¨ ë°ì´í„°ì…‹)

```python
import sklearn
print("scikit-learn version:", sklearn.__version__)
```

### ğŸ”¹ scikitâ€‘learn íŒŒì´í”„ë¼ì¸ êµ¬ì¡°
```
ë°ì´í„° âœ ì „ì²˜ë¦¬(Scaler/Encoder) âœ ëª¨ë¸ í•™ìŠµ âœ ì˜ˆì¸¡ âœ í‰ê°€
```
- **`Pipeline`** í´ë˜ìŠ¤ë¡œ ì ˆì°¨ë¥¼ **ì²´ì¸**í™”í•˜ë©´, ë°ì´í„° ëˆ„ì„¤(data leakage)ì„ ë°©ì§€í•˜ê³  í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ì„ ìë™í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ› ï¸ ì‹¤ìŠµ: ë¶“ê½ƒ(Iris) ë¶„ë¥˜

### ğŸ”¹ 1) ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
```python
from sklearn.datasets import load_iris
iris = load_iris(as_frame=True)  # DataFrame í˜•íƒœë¡œ ë°›ê¸°
X, y = iris.data, iris.target
```

### ğŸ”¹ 2) í›ˆë ¨ / í…ŒìŠ¤íŠ¸ ë¶„ë¦¬
```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
```

### ğŸ”¹ 3) ì „ì²˜ë¦¬ + ëª¨ë¸ íŒŒì´í”„ë¼ì¸
```python
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline

pipe = Pipeline([
    ("scaler", StandardScaler()),
    ("knn",    KNeighborsClassifier(n_neighbors=3))
])

pipe.fit(X_train, y_train)
```

### ğŸ”¹ 4) ì„±ëŠ¥ í‰ê°€
```python
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
y_pred = pipe.predict(X_test)

print("ì •í™•ë„:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
```

---

## ğŸ” í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

`GridSearchCV`ë¥¼ í™œìš©í•˜ë©´ `n_neighbors`, `metric` ë“± íŒŒë¼ë¯¸í„°ë¥¼ **êµì°¨ê²€ì¦**ìœ¼ë¡œ ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from sklearn.model_selection import GridSearchCV

param_grid = {"knn__n_neighbors": [1, 3, 5, 7]}
grid = GridSearchCV(pipe, param_grid, cv=5)
grid.fit(X_train, y_train)

print("ìµœì  íŒŒë¼ë¯¸í„°:", grid.best_params_)
print("ê²€ì¦ ì •í™•ë„:", grid.best_score_)
```

---

## ğŸ§ª ê³¼ì œ

> **ë°ì´í„°ì…‹**: `load_wine()` (ì™€ì¸ í’ˆì§ˆ ë°ì´í„°)

1. ì™€ì¸ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³ , KNN ë¶„ë¥˜ê¸°ë¡œ **ê¸°ë³¸ ì •í™•ë„**ë¥¼ ì¸¡ì •í•˜ì„¸ìš”.  
2. `test_size`ë¥¼ **0.3**ìœ¼ë¡œ ë³€ê²½í•´ ì •í™•ë„ ë³€í™”ë¥¼ ë¹„êµí•˜ì„¸ìš”.  
3. `n_neighbors`ë¥¼ **1, 3, 5**ë¡œ ë°”ê¾¸ì–´ ê°ê° ì •í™•ë„ë¥¼ ê¸°ë¡í•˜ì„¸ìš”.

### ğŸ”‘ íŒíŠ¸
```python
from sklearn.datasets import load_wine
from sklearn.metrics import accuracy_score

wine = load_wine(as_frame=True)
X_train, X_test, y_train, y_test = train_test_split(
    wine.data, wine.target,
    test_size=0.3, stratify=wine.target, random_state=42
)

for k in [1, 3, 5]:
    model = KNeighborsClassifier(n_neighbors=k)
    model.fit(X_train, y_train)
    acc = model.score(X_test, y_test)
    print(f"k={k}, ì •í™•ë„={acc:.3f}")
```
> **ì¶”ê°€ ë„ì „ ğŸŒŸ**Â   
> â€¢ `StandardScaler`ë¥¼ íŒŒì´í”„ë¼ì¸ì— ë„£ì–´ ì„±ëŠ¥ì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ì‚´í´ë³´ì„¸ìš”.  
> â€¢ `cross_val_score`ë¡œ ì—¬ëŸ¬ í´ë“œ í‰ê·  ì •í™•ë„ë¥¼ êµ¬í•´ë³´ì„¸ìš”.

---

## ğŸ› ï¸ scikitâ€‘learn í€µ ë ˆí¼ëŸ°ìŠ¤

| ë²”ì£¼ | ëŒ€í‘œ í´ë˜ìŠ¤ | ì„¤ëª… |
|------|-------------|------|
| ì „ì²˜ë¦¬ | `StandardScaler`, `OneHotEncoder`, `PolynomialFeatures` | ì •ê·œí™”Â·ì¸ì½”ë”©Â·íŠ¹ì„± ìƒì„± |
| ë¶„ë¥˜ | `KNeighborsClassifier`, `SVC`, `RandomForestClassifier`, `LogisticRegression` | |
| íšŒê·€ | `LinearRegression`, `SVR`, `RandomForestRegressor` | |
| í´ëŸ¬ìŠ¤í„°ë§ | `KMeans`, `DBSCAN`, `AgglomerativeClustering` | ë¹„ì§€ë„ êµ°ì§‘ |
| ì°¨ì›ì¶•ì†Œ | `PCA`, `TSNE`, `TruncatedSVD` | |
| ëª¨ë¸ ì„ íƒ | `train_test_split`, `GridSearchCV`, `cross_val_score` | ë°ì´í„° ë¶„í• Â·CVÂ·íŠœë‹ |
| í‰ê°€ ì§€í‘œ | `accuracy_score`, `mean_squared_error`, `roc_auc_score` | |

---

âœ… **ì‚¬ìš© í™˜ê²½**: PythonÂ 3.x, GoogleÂ Colab (scikitâ€‘learn ê¸°ë³¸ íƒ‘ì¬)  
í•„ìš” ì‹œ `!pip install scikit-learn -U` ë¡œ ìµœì‹  ë²„ì „ì„ ì„¤ì¹˜í•˜ì„¸ìš”.
